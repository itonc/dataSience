{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成\n",
    "集成的思想是用多个弱分类器投票的结果来替换强分类器，集成的方法有bagging,stacking 和boosting。 集成思想符合大数定理，如果每个弱分类器正确的概率51%，那么1000个分类器能判断正确的概率大于75%，10000个将到达97%，不过要求各个分类器互相独立，但是我们集成思想使用的是相同的数据，模型可能会在相同的地方犯错。\n",
    "\n",
    "在上篇文章中，我们使用逻辑回归或决策树算法进行预测，现在我们可以增加更多的模型，来看看集成是否真的有效。\n",
    "\n",
    "Logistic Regression\n",
    "KNN or k-Nearest Neighbors\n",
    "Support Vector Machines\n",
    "Naive Bayes classifier\n",
    "Decision Tree\n",
    "Perceptron\n",
    "SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_pre.csv\",index_col=0)\n",
    "test_df = pd.read_csv(\"test_pre.csv\",index_col=0)\n",
    "\n",
    "# 留0.2作为验证集\n",
    "train,verfy = train_test_split(train_df,test_size = 0.25)\n",
    "\n",
    "X_train = train.drop(\"Survived\",axis=1)\n",
    "y_train = train[\"Survived\"]\n",
    "\n",
    "X_verfy = verfy.drop(\"Survived\",axis=1)\n",
    "y_verfy = verfy[\"Survived\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.798206278027\n",
      "KNeighborsClassifier 0.838565022422\n",
      "SVC 0.811659192825\n",
      "GaussianNB 0.7533632287\n",
      "DecisionTreeClassifier 0.820627802691\n",
      "Perceptron 0.654708520179\n",
      "SGDClassifier 0.713004484305\n",
      "VotingClassifier 0.825112107623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qiucier\\Miniconda3\\envs\\py3.6\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\qiucier\\Miniconda3\\envs\\py3.6\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "kn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC()\n",
    "bayes_clf = GaussianNB()\n",
    "detree_clf = DecisionTreeClassifier()\n",
    "per_clf = Perceptron()\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"lr\",log_clf),(\"knn\",kn_clf),(\"svc\",svm_clf),(\"bayes\",bayes_clf),(\"tree\",detree_clf),(\"per\",per_clf),(\"sgd\",sgd_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "models = [log_clf,kn_clf,svm_clf,bayes_clf,detree_clf,per_clf,sgd_clf,voting_clf]\n",
    "\n",
    "\n",
    "#看集成后的分类器的表现\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "for clf in models:\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_predict = clf.predict(X_verfy)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_verfy,y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果经过多次试验你会发现根据训练集不同，单个模型中每次准确率最高的模型不一样，但是集成的模型通常是所有模型中最好或者第二好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "如果让所有弱分类器都用同一套算法，但是每次都选不同的训练集子集，采样时如果有放回，这种方法叫做Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8340807174887892"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators = 300,\n",
    "    max_samples = 320,\n",
    "    bootstrap = True,\n",
    "    n_jobs = -1)\n",
    "\n",
    "\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_pred = bag_clf.predict(X_verfy)\n",
    "accuracy_score(y_verfy,bag_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_samples对准确率有比较大的影响\n",
    "当max_samples = 80时 精确率为0.81\n",
    " max_samples = 200  精确率为0.843\n",
    " 当max_sample = 300 精确率只有0.838\n",
    " \n",
    " \n",
    " #### 有随机噪声\n",
    " 因为每次都随机采样，因此精确率不是固定不变的\n",
    " \n",
    " #### 随机特征\n",
    " 上面我们提到的例子都是随机样本，bagging也支持随机选择特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81165919282511212"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators = 300,\n",
    "    max_features = 0.9,\n",
    "    bootstrap_features = True,\n",
    "    n_jobs = -1)\n",
    "\n",
    "\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_pred = bag_clf.predict(X_verfy)\n",
    "accuracy_score(y_verfy,bag_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机特征似乎没有随机样本效果好\n",
    "\n",
    "可能跟本次的样本本身比较少有关，随机特征适用于图片这类特征非常多的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林\n",
    "\n",
    "随机森林是决策树的集成，通常用bagging（有时也可能是pasting）方法训练，训练集大小通过max_samples 来设置。随机森林在树的生长上引入了更多的随机性：分裂节点时不再是搜索最好的特征（，而是在一个随机生成的特征子集里搜索最好的特征。这导致决策树具有更大的\n",
    "多样性，（再一次）用更高的偏差换取更低的方差，总之，还是产生了一个整体性能更优的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train,y_train)\n",
    "accuracy_score(y_verfy,rf_clf.predict(X_verfy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重要性\n",
    "\n",
    "最后，如果你查看单个决策树会发现，重要的特征更可能出现在靠近根节点的位置，而不重要的特征通常出现在靠近叶节点的位置（甚至根本不出现）。因此，通过计算一个特征在森林中所有树上的平均深度，可以估算出一个特征的重要程度。\n",
    "\n",
    "如果你想分析哪个特征更重要，使用随机森林是一个不错的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass 0.113068390496\n",
      "Sex 0.226031812611\n",
      "Age 0.0550296063772\n",
      "Fare 0.118770080097\n",
      "Embarked 0.0548713239446\n",
      "Title 0.288943314477\n",
      "isAlone 0.0367610634734\n",
      "Pclass*Age 0.106524408523\n"
     ]
    }
   ],
   "source": [
    "for name,score in zip(X_train.columns,rf_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting 提升法\n",
    "\n",
    "提升法是将几个弱学习器变成一个强分类器，提升的意思是循环训练预测器，每次都根据前序的结果做出修改，可用的提升方法比较多，目前常用的是自适应提升Adaboost和梯度提升\n",
    "\n",
    "### Adboost\n",
    "\n",
    "   第一个分类器产生了许多错误实例， 所以这些实例的权重得到提升。因此第二个分类器在这些实例上的表现有所提升，然后第三个、第四个……右图绘制的是相同预测器序列，唯一的差别在于学习率减半（即每次迭代仅提升一半错误分类的实例的权重）。可以看出， AdaBoost 这种依序循环的学习技术跟梯度下降有一些异曲同工之处，差别只在于一一不再是调整单个预测器的参数使成本函数最小化，而是不断在集成中加入预测器，使模型越来越好。\n",
    "  一旦全部预测器训练完成，集成整体做出预测时就跟bagging 方法一样了，除非预测器有不同的权重，因为它们总的准确率是基于加权后的训练集。\n",
    "  \n",
    " 这种方法有一个重要缺陷，不能并行，因为必须等一个模型训练结束了才知道下一个模型应该怎么训练，因此在拓展性方面不如bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80269058295964124"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1) , n_estimators=400,\n",
    "algorithm=\"SAMME.R\" ,learning_rate=0.5)\n",
    "ada_clf.fit(X_train,y_train)\n",
    "accuracy_score(y_verfy,ada_clf.predict(X_verfy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost 效果比随机森林还要差一点，跟普通的单个分类器差别不大\n",
    "\n",
    "adaboost的结果是稳定的，没有随机噪声\n",
    "\n",
    "#### 调参\n",
    "如果过拟合，可以减少分类器，或者提高基础分类器的正则化程度 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度提升 GBDT\n",
    "\n",
    "跟AdaBoost 一样，梯度提升也是逐步集成预测其，下一步是对上一步的改进，与adaboost不同的是不是调整实例的权重，而是让新的预测器对前一个预测结果的残差进行拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81614349775784756"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "gb_clf.fit(X_train,y_train)\n",
    "accuracy_score(y_verfy,gb_clf.predict(X_verfy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT似乎是稳定的\n",
    "不会每次运行的结果不一样\n",
    "\n",
    "#### 调参\n",
    "如果过拟合，可以减少预测器个数，也可以降低学习速率，或者通过提前停止来减少分类器的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 堆叠法stacking\n",
    "\n",
    "堆叠法就是在bagging中，是多个模型投票，而对于堆叠来说，改为用一个新的预测把多个模型的预测结果作为输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qiucier\\Miniconda3\\envs\\py3.6\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\qiucier\\Miniconda3\\envs\\py3.6\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "kn_clf = KNeighborsClassifier()\n",
    "svm_clf = SVC()\n",
    "bayes_clf = GaussianNB()\n",
    "detree_clf = DecisionTreeClassifier()\n",
    "per_clf = Perceptron()\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = [log_clf,kn_clf,svm_clf,bayes_clf,detree_clf,per_clf,sgd_clf]\n",
    "\n",
    "\n",
    "#看集成后的分类器的表现\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "stack_train_df = pd.DataFrame(np.zeros(668),columns=['zero'])\n",
    "stack_verfy_df = pd.DataFrame(np.zeros(223),columns=['zero'])\n",
    "\n",
    "for clf in models:\n",
    "    clf.fit(X_train,y_train)\n",
    "    stack_train_df[clf.__class__.__name__] = pd.Series(clf.predict(X_train))\n",
    "    stack_verfy_df[clf.__class__.__name__] = pd.Series(clf.predict(X_verfy))\n",
    "\n",
    "\n",
    "## 构建出新的训练集和验证集\n",
    "stack_train_df.drop('zero',axis=1)\n",
    "stack_verfy_df.drop('zero',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.829596412556\n",
      "KNeighborsClassifier 0.829596412556\n",
      "SVC 0.820627802691\n",
      "GaussianNB 0.820627802691\n",
      "DecisionTreeClassifier 0.820627802691\n",
      "Perceptron 0.784753363229\n",
      "SGDClassifier 0.820627802691\n"
     ]
    }
   ],
   "source": [
    "for clf in models:\n",
    "    clf.fit(stack_train_df,y_train)\n",
    "    y_predict = clf.predict(stack_verfy_df)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_verfy,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack 比单层模型稳定性有提升\n",
    "以多个弱分类器的结果作为输入后，第二层分类器不论是哪种，稳定性都比较好，方差比较小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
